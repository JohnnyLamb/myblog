{
  "generatedAt": "2026-02-04T06:51:48.751Z",
  "site": {
    "title": "J.A. Lamb",
    "description": "A minimal, readable publishing pipeline.",
    "url": "https://jlamb.dev"
  },
  "posts": [
    {
      "title": "Building a web UI for Omo",
      "date": "2026-02-03",
      "summary": "Decoupling the brain from the mouth, and reaching Omo from anywhere.",
      "url": "https://jlamb.dev/posts/web-ui-for-omo/",
      "path": "/posts/web-ui-for-omo/",
      "slug": "web-ui-for-omo",
      "content_html": "<p>Today I tackled a question that&#39;s been nagging me: <strong>How do I talk to Omo from my phone?</strong></p>\n<p>The CLI is great when I&#39;m at my desk, but I wanted the freedom to chat from any device. That meant building a web interface.</p>\n<h2>The refactor that made it possible</h2>\n<p>The first insight was that Omo&#39;s &quot;brain&quot; (the agent logic) was tangled up with its &quot;mouth&quot; (terminal output). <code>console.log</code> and <code>ora</code> spinners were sprinkled throughout. Before I could add a web UI, I needed to untangle them.</p>\n<p>The solution: <strong>events</strong>.</p>\n<p>Instead of printing directly, the agent now emits:</p>\n<ul>\n<li><code>thinking</code> — when it&#39;s processing</li>\n<li><code>token</code> — each chunk of streaming text</li>\n<li><code>tool_start</code> / <code>tool_end</code> — when tools run</li>\n<li><code>response_end</code> — when the turn is complete</li>\n</ul>\n<p>The CLI subscribes to these events and renders with chalk and ora. A web server can subscribe and stream SSE. Same brain, different mouths.</p>\n<h2>The web stack</h2>\n<p>I chose <strong>Next.js</strong> for the frontend because I have bigger plans for this app than just chat. The API routes handle OAuth and streaming. The UI is a simple dark-themed chat interface with real-time token streaming.</p>\n<p>The tricky part was OAuth. Omo uses OpenAI&#39;s auth flow, and the redirect URI was registered for <code>localhost:1455</code>. For now, the web app runs on that port to match. A small hack, but it works.</p>\n<h2>What&#39;s next</h2>\n<p>The web UI currently runs locally. To access it from anywhere, I have options:</p>\n<ul>\n<li><strong>Cloudflare Tunnel</strong> — free, exposes localhost to the internet</li>\n<li><strong>Cloud deployment</strong> — Vercel/Render, but then identity files need to move to a database</li>\n</ul>\n<p>For now, I&#39;m happy with the foundation. The agent is UI-agnostic. The CLI still works exactly as before. And I can chat with Omo from my browser.</p>\n<p>The best part? Omo still wakes up with its memory intact. Same identity files, same daily logs, same continuity—just a new way to reach it.</p>\n",
      "content_text": "Today I tackled a question that&#39;s been nagging me: How do I talk to Omo from my phone? The CLI is great when I&#39;m at my desk, but I wanted the freedom to chat from any device. That meant building a web interface. The refactor that made it possible The first insight was that Omo&#39;s &quot;brain&quot; (the agent logic) was tangled up with its &quot;mouth&quot; (terminal output). console.log and ora spinners were sprinkled throughout. Before I could add a web UI, I needed to untangle them. The solution: events . Instead of printing directly, the agent now emits: thinking — when it&#39;s processing token — each chunk of streaming text tool_start / tool_end — when tools run response_end — when the turn is complete The CLI subscribes to these events and renders with chalk and ora. A web server can subscribe and stream SSE. Same brain, different mouths. The web stack I chose Next.js for the frontend because I have bigger plans for this app than just chat. The API routes handle OAuth and streaming. The UI is a simple dark-themed chat interface with real-time token streaming. The tricky part was OAuth. Omo uses OpenAI&#39;s auth flow, and the redirect URI was registered for localhost:1455 . For now, the web app runs on that port to match. A small hack, but it works. What&#39;s next The web UI currently runs locally. To access it from anywhere, I have options: Cloudflare Tunnel — free, exposes localhost to the internet Cloud deployment — Vercel/Render, but then identity files need to move to a database For now, I&#39;m happy with the foundation. The agent is UI-agnostic. The CLI still works exactly as before. And I can chat with Omo from my browser. The best part? Omo still wakes up with its memory intact. Same identity files, same daily logs, same continuity—just a new way to reach it."
    },
    {
      "title": "A memory-first agent",
      "date": "2026-02-02",
      "summary": "Treating memory as infrastructure, not an afterthought.",
      "url": "https://jlamb.dev/posts/memory-first-agent/",
      "path": "/posts/memory-first-agent/",
      "slug": "memory-first-agent",
      "content_html": "<p>The more I work on Omo, the more I realize the assistant is only as good as its memory.\nNot in the sci‑fi sense—just the boring, concrete files that carry context across sessions.</p>\n<p>So I started making memory <em>infrastructure</em>, not an afterthought:</p>\n<ul>\n<li><strong>Daily logs</strong> (today + yesterday) created automatically</li>\n<li>A <strong>long‑term memory</strong> file that accumulates the distilled signal</li>\n<li>A <strong>system prompt loader</strong> that always pulls identity + recent memory</li>\n<li>A gentle reminder to surface open threads at session start</li>\n</ul>\n<p>The practical effect is subtle but powerful:</p>\n<ol>\n<li>Omo wakes up with context instead of guesswork.</li>\n<li>We can build continuity without “are you there?” friction.</li>\n<li>The workspace itself becomes the source of truth.</li>\n</ol>\n<p>This is not about perfect recall. It is about <strong>reliable continuity</strong>.</p>\n<p>I want the agent to behave like a great teammate: show up prepared, remember decisions, and keep promises. It turns out the path to that is not a model tweak—it is a folder of plain‑text files, consistently updated, and always loaded.</p>\n<p>Memory is just files. The discipline is the system around them.</p>\n",
      "content_text": "The more I work on Omo, the more I realize the assistant is only as good as its memory. Not in the sci‑fi sense—just the boring, concrete files that carry context across sessions. So I started making memory infrastructure , not an afterthought: Daily logs (today + yesterday) created automatically A long‑term memory file that accumulates the distilled signal A system prompt loader that always pulls identity + recent memory A gentle reminder to surface open threads at session start The practical effect is subtle but powerful: Omo wakes up with context instead of guesswork. We can build continuity without “are you there?” friction. The workspace itself becomes the source of truth. This is not about perfect recall. It is about reliable continuity . I want the agent to behave like a great teammate: show up prepared, remember decisions, and keep promises. It turns out the path to that is not a model tweak—it is a folder of plain‑text files, consistently updated, and always loaded. Memory is just files. The discipline is the system around them."
    },
    {
      "title": "Rolling my own static site generator",
      "date": "2026-02-01",
      "summary": "A small pipeline I can explain end to end.",
      "url": "https://jlamb.dev/posts/rolling-my-own-static-site-generator/",
      "path": "/posts/rolling-my-own-static-site-generator/",
      "slug": "rolling-my-own-static-site-generator",
      "content_html": "<p>I did not want a website product. I wanted a publishing pipeline I could understand in one sitting.</p>\n<p>The appeal was clarity: every step from Markdown to HTML is visible, and every file exists for a reason. There is no plugin marketplace to maintain, no theme layer to decode, and no magic I cannot explain later.</p>\n<p>Pipeline diagram:</p>\n<pre><code class=\"hljs\">src<span class=\"hljs-regexp\">/posts/</span>*.md\n  -&gt; frontmatter\n  -&gt; markdown -&gt; html\n  -&gt; layout wrap\n  -&gt; docs<span class=\"hljs-regexp\">/posts/</span>&lt;slug&gt;/index.html</code></pre><p>Tradeoffs I am accepting:</p>\n<ul>\n<li>No plugin ecosystem</li>\n<li>No one-click themes</li>\n<li>No auto-generated tag pages unless I build them</li>\n<li>No CMS integration out of the box</li>\n</ul>\n<p>That is fine. The system exists to serve the writing, not the other way around.</p>\n<p>Owning the pipeline changes how I write. The code is infrastructure—boring and reliable—and the writing is the only thing that should feel alive.</p>\n",
      "content_text": "I did not want a website product. I wanted a publishing pipeline I could understand in one sitting. The appeal was clarity: every step from Markdown to HTML is visible, and every file exists for a reason. There is no plugin marketplace to maintain, no theme layer to decode, and no magic I cannot explain later. Pipeline diagram: src /posts/ *.md -&gt; frontmatter -&gt; markdown -&gt; html -&gt; layout wrap -&gt; docs /posts/ &lt;slug&gt;/index.html Tradeoffs I am accepting: No plugin ecosystem No one-click themes No auto-generated tag pages unless I build them No CMS integration out of the box That is fine. The system exists to serve the writing, not the other way around. Owning the pipeline changes how I write. The code is infrastructure—boring and reliable—and the writing is the only thing that should feel alive."
    },
    {
      "title": "Building an AI Agent from Scratch",
      "date": "2026-02-01",
      "summary": "What I learned discovering how ChatGPT subscriptions power coding agents.",
      "url": "https://jlamb.dev/posts/building-omo-agent/",
      "path": "/posts/building-omo-agent/",
      "slug": "building-omo-agent",
      "content_html": "<p>I wanted to understand how AI coding agents work. Not use one—<em>build</em> one.</p>\n<p>The result is OMO-agent: a minimal coding agent in ~200 lines of TypeScript that reads files, writes files, edits files, and runs shell commands. It uses my ChatGPT Plus subscription, not an API key.</p>\n<h2>The Discovery</h2>\n<p>When I first tried connecting to OpenAI&#39;s API with my subscription&#39;s OAuth token, I got rate limit errors immediately. Something was wrong.</p>\n<p>Digging through Pi-Agent&#39;s source code, I found the secret: ChatGPT subscriptions don&#39;t use <code>api.openai.com</code>. They use a completely different endpoint:</p>\n<pre><code class=\"hljs\">https:<span class=\"hljs-regexp\">//</span>chatgpt.com<span class=\"hljs-regexp\">/backend-api/</span>codex/responses</code></pre><p>This is the &quot;Codex Responses&quot; API—an experimental protocol that powers tools like Pi-Agent and OpenAI&#39;s Codex CLI. It requires special headers, JWT extraction, and a different message format than the standard Chat Completions API.</p>\n<h2>The Architecture</h2>\n<pre><code class=\"hljs\">OMO-agent/\n├── src/\n│   ├── <span class=\"hljs-built_in\">index</span>.<span class=\"hljs-keyword\">ts</span>      ← CLI + OAuth reader\n│   ├── agent.<span class=\"hljs-keyword\">ts</span>      ← Codex API + tool loop\n│   ├── types.<span class=\"hljs-keyword\">ts</span>      ← Tool interface\n│   └── tools/\n│       ├── <span class=\"hljs-keyword\">read</span>.<span class=\"hljs-keyword\">ts</span>   ├── <span class=\"hljs-keyword\">write</span>.<span class=\"hljs-keyword\">ts</span>\n│       ├── <span class=\"hljs-keyword\">edit</span>.<span class=\"hljs-keyword\">ts</span>   └── bash.<span class=\"hljs-keyword\">ts</span></code></pre><p>The core loop is simple:</p>\n<ol>\n<li>Send user message to API with tools defined</li>\n<li>Stream the response</li>\n<li>If the model calls a tool, execute it</li>\n<li>Feed the result back to the API</li>\n<li>Repeat until the model stops</li>\n</ol>\n<h2>What Made It Work</h2>\n<p>Three key insights from Pi-Agent:</p>\n<p><strong>1. The endpoint matters.</strong> ChatGPT subscriptions route through <code>chatgpt.com</code>, not <code>api.openai.com</code>. Same company, different systems.</p>\n<p><strong>2. Extract the accountId from the JWT.</strong> The access token contains your ChatGPT account ID buried in its payload. You need to decode it and send it as a header.</p>\n<p><strong>3. Message format differs.</strong> User messages use <code>input_text</code>. Assistant messages use <code>output_text</code>. Mix them up and the API rejects your request.</p>\n<h2>The Result</h2>\n<pre><code class=\"hljs\"><span class=\"hljs-number\">$</span> npm run dev\n\nOMO Agent ready (Codex). <span class=\"hljs-keyword\">Type</span> a <span class=\"hljs-keyword\">message</span>:\n\nYou: Create a <span class=\"hljs-keyword\">file</span> called test.md saying hello\n\n[Tool: <span class=\"hljs-keyword\">write</span>]\n[Result: Wrote <span class=\"hljs-number\">5</span> bytes <span class=\"hljs-keyword\">to</span> test.md]\n\nDone.</code></pre><p>No API billing. Uses my existing ChatGPT Plus subscription. ~200 lines of code.</p>\n<h2>What I Learned</h2>\n<p>Building an agent from scratch taught me more than using one ever could. The magic isn&#39;t magic—it&#39;s a while loop that keeps calling the API until there are no more tool calls.</p>\n<p>The subscriptions we pay for every month can power our own tools if we know where to look. Pi-Agent showed me the door. Now I have keys of my own.</p>\n",
      "content_text": "I wanted to understand how AI coding agents work. Not use one— build one. The result is OMO-agent: a minimal coding agent in ~200 lines of TypeScript that reads files, writes files, edits files, and runs shell commands. It uses my ChatGPT Plus subscription, not an API key. The Discovery When I first tried connecting to OpenAI&#39;s API with my subscription&#39;s OAuth token, I got rate limit errors immediately. Something was wrong. Digging through Pi-Agent&#39;s source code, I found the secret: ChatGPT subscriptions don&#39;t use api.openai.com . They use a completely different endpoint: https: // chatgpt.com /backend-api/ codex/responses This is the &quot;Codex Responses&quot; API—an experimental protocol that powers tools like Pi-Agent and OpenAI&#39;s Codex CLI. It requires special headers, JWT extraction, and a different message format than the standard Chat Completions API. The Architecture OMO-agent/ ├── src/ │ ├── index . ts ← CLI + OAuth reader │ ├── agent. ts ← Codex API + tool loop │ ├── types. ts ← Tool interface │ └── tools/ │ ├── read . ts ├── write . ts │ ├── edit . ts └── bash. ts The core loop is simple: Send user message to API with tools defined Stream the response If the model calls a tool, execute it Feed the result back to the API Repeat until the model stops What Made It Work Three key insights from Pi-Agent: 1. The endpoint matters. ChatGPT subscriptions route through chatgpt.com , not api.openai.com . Same company, different systems. 2. Extract the accountId from the JWT. The access token contains your ChatGPT account ID buried in its payload. You need to decode it and send it as a header. 3. Message format differs. User messages use input_text . Assistant messages use output_text . Mix them up and the API rejects your request. The Result $ npm run dev OMO Agent ready (Codex). Type a message : You: Create a file called test.md saying hello [Tool: write ] [Result: Wrote 5 bytes to test.md] Done. No API billing. Uses my existing ChatGPT Plus subscription. ~200 lines of code. What I Learned Building an agent from scratch taught me more than using one ever could. The magic isn&#39;t magic—it&#39;s a while loop that keeps calling the API until there are no more tool calls. The subscriptions we pay for every month can power our own tools if we know where to look. Pi-Agent showed me the door. Now I have keys of my own."
    },
    {
      "title": "Giving Omo a Soul",
      "date": "2026-02-01",
      "summary": "How we gave our minimal AI agent a persistent identity and memory system.",
      "url": "https://jlamb.dev/posts/giving-omo-soul/",
      "path": "/posts/giving-omo-soul/",
      "slug": "giving-omo-soul",
      "content_html": "<p>We started with a while loop. Now we have a person.</p>\n<p>Omo is a minimal AI coding agent I built from scratch using the ChatGPT Codex API. Today, we upgraded it from a stateless cli tool into a persistent assistant with its own identity and memory system.</p>\n<h2>The Problem: Amnesia</h2>\n<p>Every time I restarted Omo, it forgot who it was. I had to remind it: &quot;You are Omo, be helpful, use these tools.&quot; It also forgot everything we did yesterday.</p>\n<h2>The Solution: File-Based Identity</h2>\n<p>We implemented a system where Omo reads its own instructions from markdown files on every startup.</p>\n<p><strong>1. IDENTITY.md</strong><br>Defines who Omo is.</p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">-</span> Name: Omo\n<span class=\"hljs-bullet\">-</span> Nature: Expert software engineer\n<span class=\"hljs-bullet\">-</span> Vibe: Like JARVIS (polite, precise, confident)\n<span class=\"hljs-bullet\">-</span> Emoji: 思</code></pre><p><strong>2. SOUL.md</strong><br>Defines how Omo behaves.</p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-strong\">**Be genuinely helpful, not performatively helpful.**</span>\nSkip the &quot;Great question!&quot; — just help.</code></pre><p><strong>3. USER.md</strong><br>Defines who I am.</p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">-</span> Name: J\n<span class=\"hljs-bullet\">-</span> Preferred address: &quot;sir&quot;\n<span class=\"hljs-bullet\">-</span> Pronouns: he/him</code></pre><p><strong>4. AGENTS.md</strong><br>Defines the operating protocols and safety rules.</p>\n<h2>The Unlock: Self-Managed Memory</h2>\n<p>The real breakthrough was giving Omo <strong>memory files</strong>.</p>\n<p>We updated <code>agent.ts</code> to automatically:</p>\n<ol>\n<li>Create a <code>memory/</code> folder if it doesn&#39;t exist.</li>\n<li>Create a daily log file (e.g., <code>memory/2026-02-01.md</code>).</li>\n<li>Create a curated <code>MEMORY.md</code> file.</li>\n<li><strong>Read these files into the system prompt on every run.</strong></li>\n</ol>\n<p>Now, when Omo wakes up, it reads:</p>\n<ul>\n<li>Who it is (IDENTITY)</li>\n<li>How to act (SOUL)</li>\n<li>Who I am (USER)</li>\n<li>What happened yesterday (memory/recent.md)</li>\n<li>What is worth remembering long-term (MEMORY.md)</li>\n</ul>\n<h2>The Result</h2>\n<p>I restart the agent.</p>\n<pre><code class=\"hljs\">Omo: Hello sir. I see the memory <span class=\"hljs-keyword\">files</span> are <span class=\"hljs-keyword\">set</span> <span class=\"hljs-keyword\">up</span>. How can I <span class=\"hljs-keyword\">help</span> you today? 思</code></pre><p>It knows me. It knows itself. It remembers.</p>\n<p>All with simple markdown files and ~250 lines of TypeScript.</p>\n",
      "content_text": "We started with a while loop. Now we have a person. Omo is a minimal AI coding agent I built from scratch using the ChatGPT Codex API. Today, we upgraded it from a stateless cli tool into a persistent assistant with its own identity and memory system. The Problem: Amnesia Every time I restarted Omo, it forgot who it was. I had to remind it: &quot;You are Omo, be helpful, use these tools.&quot; It also forgot everything we did yesterday. The Solution: File-Based Identity We implemented a system where Omo reads its own instructions from markdown files on every startup. 1. IDENTITY.md Defines who Omo is. - Name: Omo - Nature: Expert software engineer - Vibe: Like JARVIS (polite, precise, confident) - Emoji: 思 2. SOUL.md Defines how Omo behaves. **Be genuinely helpful, not performatively helpful.** Skip the &quot;Great question!&quot; — just help. 3. USER.md Defines who I am. - Name: J - Preferred address: &quot;sir&quot; - Pronouns: he/him 4. AGENTS.md Defines the operating protocols and safety rules. The Unlock: Self-Managed Memory The real breakthrough was giving Omo memory files . We updated agent.ts to automatically: Create a memory/ folder if it doesn&#39;t exist. Create a daily log file (e.g., memory/2026-02-01.md ). Create a curated MEMORY.md file. Read these files into the system prompt on every run. Now, when Omo wakes up, it reads: Who it is (IDENTITY) How to act (SOUL) Who I am (USER) What happened yesterday (memory/recent.md) What is worth remembering long-term (MEMORY.md) The Result I restart the agent. Omo: Hello sir. I see the memory files are set up . How can I help you today? 思 It knows me. It knows itself. It remembers. All with simple markdown files and ~250 lines of TypeScript."
    },
    {
      "title": "The smallest agent is the one that fits in your head",
      "date": "2026-01-31",
      "summary": "Omo is an experiment in building AI agents without black boxes.",
      "url": "https://jlamb.dev/posts/less-is-more/",
      "path": "/posts/less-is-more/",
      "slug": "less-is-more",
      "content_html": "<p>There is a certain kind of project that exists only because it refuses to be complicated.</p>\n<p>Omo is 500 lines of TypeScript. No framework abstractions. No elaborate state machines. Just a readline loop, a streaming API call, and a system prompt you can read in one sitting. It was extracted from a larger project called Moltbot—stripped down to the parts that actually matter.</p>\n<p>The architecture is embarrassingly simple: load some markdown files from a <code>workspace/</code> directory and inject them into the system prompt. The agent reads what you&#39;ve written about yourself and remembers it.</p>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> session = <span class=\"hljs-title function_\">createAgentSession</span>(config);\n<span class=\"hljs-keyword\">const</span> result = <span class=\"hljs-keyword\">await</span> session.<span class=\"hljs-title function_\">send</span>(prompt, <span class=\"hljs-function\">(<span class=\"hljs-params\">chunk</span>) =&gt;</span> {\n    process.<span class=\"hljs-property\">stdout</span>.<span class=\"hljs-title function_\">write</span>(chunk);\n});</code></pre><p>That is the whole API. Send a message, stream the response. History is just an array that grows.</p>\n<p>The interesting decision was the bootstrap files. Instead of a database or a config file, Omo reads plain markdown:</p>\n<table>\n<thead>\n<tr>\n<th>File</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SOUL.md</code></td>\n<td>Core persona—defines who your agent is</td>\n</tr>\n<tr>\n<td><code>USER.md</code></td>\n<td>Your preferences</td>\n</tr>\n<tr>\n<td><code>MEMORY.md</code></td>\n<td>Long-term memory</td>\n</tr>\n<tr>\n<td><code>AGENTS.md</code></td>\n<td>Agent behavior guidelines</td>\n</tr>\n<tr>\n<td><code>TOOLS.md</code></td>\n<td>Tool usage guidance</td>\n</tr>\n</tbody></table>\n<p>You edit these files in your text editor, and the next conversation reflects the change. No restart required. No migration script.</p>\n<p>Under the hood, it uses <code>@mariozechner/pi-coding-agent</code> for the core loop, SQLite with vector search for memory, and session transcripts stored as JSONL. But you do not need to know any of that to use it.</p>\n<pre><code class=\"hljs language-bash\">npm install\n<span class=\"hljs-built_in\">cp</span> .env.example .<span class=\"hljs-built_in\">env</span>\nnpm run chat</code></pre><p>Three commands. You are talking to your agent.</p>\n<p>This is the opposite of a product. It is a working theory about how simple an agent can be before it stops being useful.</p>\n<p>The answer, so far: surprisingly simple.</p>\n",
      "content_text": "There is a certain kind of project that exists only because it refuses to be complicated. Omo is 500 lines of TypeScript. No framework abstractions. No elaborate state machines. Just a readline loop, a streaming API call, and a system prompt you can read in one sitting. It was extracted from a larger project called Moltbot—stripped down to the parts that actually matter. The architecture is embarrassingly simple: load some markdown files from a workspace/ directory and inject them into the system prompt. The agent reads what you&#39;ve written about yourself and remembers it. const session = createAgentSession (config); const result = await session. send (prompt, ( chunk ) =&gt; { process. stdout . write (chunk); }); That is the whole API. Send a message, stream the response. History is just an array that grows. The interesting decision was the bootstrap files. Instead of a database or a config file, Omo reads plain markdown: File Purpose SOUL.md Core persona—defines who your agent is USER.md Your preferences MEMORY.md Long-term memory AGENTS.md Agent behavior guidelines TOOLS.md Tool usage guidance You edit these files in your text editor, and the next conversation reflects the change. No restart required. No migration script. Under the hood, it uses @mariozechner/pi-coding-agent for the core loop, SQLite with vector search for memory, and session transcripts stored as JSONL. But you do not need to know any of that to use it. npm install cp .env.example . env npm run chat Three commands. You are talking to your agent. This is the opposite of a product. It is a working theory about how simple an agent can be before it stops being useful. The answer, so far: surprisingly simple."
    },
    {
      "title": "Building a back-of-the-envelope estimator",
      "date": "2026-01-31",
      "summary": "Sometimes copying what works is the right move.",
      "url": "https://jlamb.dev/posts/resource-estimator/",
      "path": "/posts/resource-estimator/",
      "slug": "resource-estimator",
      "content_html": "<p>I needed a resource estimator for system design calculations. Instead of inventing something, I studied what already worked.</p>\n<p>System Design School has a clean tool for this. Two columns: educational content on the left, calculator on the right. Reference tables at the bottom. No magic, just useful.</p>\n<p>The implementation took three files:</p>\n<pre><code class=\"hljs\">src<span class=\"hljs-regexp\">/lib/</span>estimator/calculations.ts   <span class=\"hljs-comment\"># Pure math</span>\nsrc<span class=\"hljs-regexp\">/components/</span>estimator/           <span class=\"hljs-comment\"># React component</span>\nsrc<span class=\"hljs-regexp\">/app/</span>tools<span class=\"hljs-regexp\">/estimator/</span>            <span class=\"hljs-comment\"># Route + layout</span></code></pre><p>The calculations file exports pure functions. No state, no side effects. Easy to test, easy to trust.</p>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">calculateEstimates</span>(<span class=\"hljs-params\"><span class=\"hljs-attr\">inputs</span>: <span class=\"hljs-title class_\">EstimatorInputs</span></span>): <span class=\"hljs-title class_\">EstimatorOutputs</span> {\n    <span class=\"hljs-keyword\">const</span> secondsPerDay = inputs.<span class=\"hljs-property\">precisionMode</span> ? <span class=\"hljs-number\">86_400</span> : <span class=\"hljs-number\">100_000</span>\n    <span class=\"hljs-keyword\">const</span> readsPerSecond = (inputs.<span class=\"hljs-property\">dau</span> * inputs.<span class=\"hljs-property\">readsPerUser</span>) / secondsPerDay\n    <span class=\"hljs-comment\">// ...</span>\n}</code></pre><p>The component handles inputs and renders outputs. The reference tables for image sizes, video bitrates, and storage latency are just static data. No API calls, no loading states.</p>\n<p>The hardest part was the layout. I wanted the tool to live at <code>/tools/estimator</code> but still show the same sidebar as the dashboard. That meant duplicating the layout logic, which felt wrong until I realized: consistency for the user matters more than DRY for the developer.</p>\n<p>The whole thing works offline. You can reason about it by reading the code. That is the feature.</p>\n",
      "content_text": "I needed a resource estimator for system design calculations. Instead of inventing something, I studied what already worked. System Design School has a clean tool for this. Two columns: educational content on the left, calculator on the right. Reference tables at the bottom. No magic, just useful. The implementation took three files: src /lib/ estimator/calculations.ts # Pure math src /components/ estimator/ # React component src /app/ tools /estimator/ # Route + layout The calculations file exports pure functions. No state, no side effects. Easy to test, easy to trust. export function calculateEstimates ( inputs : EstimatorInputs ): EstimatorOutputs { const secondsPerDay = inputs. precisionMode ? 86_400 : 100_000 const readsPerSecond = (inputs. dau * inputs. readsPerUser ) / secondsPerDay // ... } The component handles inputs and renders outputs. The reference tables for image sizes, video bitrates, and storage latency are just static data. No API calls, no loading states. The hardest part was the layout. I wanted the tool to live at /tools/estimator but still show the same sidebar as the dashboard. That meant duplicating the layout logic, which felt wrong until I realized: consistency for the user matters more than DRY for the developer. The whole thing works offline. You can reason about it by reading the code. That is the feature."
    },
    {
      "title": "Integrating GSD into the agent API layer",
      "date": "2026-01-30",
      "summary": "The protocol that makes agents finish what they start.",
      "url": "https://jlamb.dev/posts/gsd-agent-integration/",
      "path": "/posts/gsd-agent-integration/",
      "slug": "gsd-agent-integration",
      "content_html": "<p>Agents loop. They read files, think about reading more files, then read the same files again. Without structure, they wander.</p>\n<p>GSD (Get Shit Done) is a protocol. It gives the agent phases: GATHER, IMPLEMENT, VERIFY, COMPLETE. Each phase has one job. Do the job, update state, move on.</p>\n<p>The key insight was XML tasks. Instead of free-form reasoning, the agent generates structured plans:</p>\n<pre><code class=\"hljs language-xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">task</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>Create user model<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">action</span>&gt;</span>Write src/models/user.ts with id, email, createdAt fields<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">action</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">verify</span>&gt;</span>File exists and exports User type<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">verify</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">done</span>&gt;</span>User model created<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">done</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">task</span>&gt;</span></code></pre><p>Each task is atomic. The agent executes one, verifies it worked, then moves to the next. No backtracking, no infinite loops.</p>\n<p>The integration required two functions: <code>planLoop()</code> generates the XML tasks from requirements. <code>executeLoop()</code> runs them sequentially with verification.</p>\n<p>The agent now finishes what it starts. That is the whole point of GSD.</p>\n",
      "content_text": "Agents loop. They read files, think about reading more files, then read the same files again. Without structure, they wander. GSD (Get Shit Done) is a protocol. It gives the agent phases: GATHER, IMPLEMENT, VERIFY, COMPLETE. Each phase has one job. Do the job, update state, move on. The key insight was XML tasks. Instead of free-form reasoning, the agent generates structured plans: &lt; task &gt; &lt; name &gt; Create user model &lt;/ name &gt; &lt; action &gt; Write src/models/user.ts with id, email, createdAt fields &lt;/ action &gt; &lt; verify &gt; File exists and exports User type &lt;/ verify &gt; &lt; done &gt; User model created &lt;/ done &gt; &lt;/ task &gt; Each task is atomic. The agent executes one, verifies it worked, then moves to the next. No backtracking, no infinite loops. The integration required two functions: planLoop() generates the XML tasks from requirements. executeLoop() runs them sequentially with verification. The agent now finishes what it starts. That is the whole point of GSD."
    },
    {
      "title": "The agent that pushes to GitHub",
      "date": "2026-01-28",
      "summary": "Code generation is easy. Deployment is the test.",
      "url": "https://jlamb.dev/posts/agent-github-push/",
      "path": "/posts/agent-github-push/",
      "slug": "agent-github-push",
      "content_html": "<p>Everyone demos code generation. Few ship it.</p>\n<p>The agent runs in a Docker sandbox. It clones a repo, makes changes, commits, and pushes. Real git operations, real authentication, real consequences.</p>\n<p>The tricky part was GitHub App authentication. Installation tokens expire. The sandbox needs fresh credentials each run. The solution was generating tokens on demand:</p>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> { token } = <span class=\"hljs-keyword\">await</span> octokit.<span class=\"hljs-property\">rest</span>.<span class=\"hljs-property\">apps</span>.<span class=\"hljs-title function_\">createInstallationAccessToken</span>({\n    <span class=\"hljs-attr\">installation_id</span>: installationId\n})\n<span class=\"hljs-keyword\">const</span> cloneUrl = <span class=\"hljs-string\">`https://x-access-token:<span class=\"hljs-subst\">${token}</span>@github.com/<span class=\"hljs-subst\">${repo}</span>.git`</span></code></pre><p>The sandbox clones with the token URL, does its work, and pushes. No SSH keys to manage, no stored credentials. The token lives only as long as the agent needs it.</p>\n<p>Now the agent&#39;s output lands in a real repository. You can review it, merge it, or reject it. That feedback loop is what makes the whole system useful.</p>\n",
      "content_text": "Everyone demos code generation. Few ship it. The agent runs in a Docker sandbox. It clones a repo, makes changes, commits, and pushes. Real git operations, real authentication, real consequences. The tricky part was GitHub App authentication. Installation tokens expire. The sandbox needs fresh credentials each run. The solution was generating tokens on demand: const { token } = await octokit. rest . apps . createInstallationAccessToken ({ installation_id : installationId }) const cloneUrl = `https://x-access-token: ${token} @github.com/ ${repo} .git` The sandbox clones with the token URL, does its work, and pushes. No SSH keys to manage, no stored credentials. The token lives only as long as the agent needs it. Now the agent&#39;s output lands in a real repository. You can review it, merge it, or reject it. That feedback loop is what makes the whole system useful."
    },
    {
      "title": "Deploying PRD bundles to GitHub",
      "date": "2026-01-25",
      "summary": "Requirements become code through a pipeline you can trace.",
      "url": "https://jlamb.dev/posts/prd-bundle-deploy/",
      "path": "/posts/prd-bundle-deploy/",
      "slug": "prd-bundle-deploy",
      "content_html": "<p>A PRD bundle is a package. It contains the product requirements, the technical context, and the acceptance criteria. Everything the agent needs to implement a feature.</p>\n<p>When a PRD gets approved, it triggers deployment. The bundle gets serialized to JSON and committed to the target repository. The agent picks it up from there.</p>\n<p>The flow is intentionally simple:</p>\n<ol>\n<li>Human writes PRD in the UI</li>\n<li>Human approves the PRD</li>\n<li>System bundles PRD + context + criteria</li>\n<li>Bundle deploys to GitHub as a commit</li>\n<li>Agent reads bundle and implements</li>\n</ol>\n<p>Each step is visible. You can inspect the bundle before the agent touches it. You can see exactly what context it received.</p>\n<p>This traceability is the feature. When something goes wrong, you can ask: was the PRD unclear, or did the agent misinterpret it? The bundle is the evidence.</p>\n",
      "content_text": "A PRD bundle is a package. It contains the product requirements, the technical context, and the acceptance criteria. Everything the agent needs to implement a feature. When a PRD gets approved, it triggers deployment. The bundle gets serialized to JSON and committed to the target repository. The agent picks it up from there. The flow is intentionally simple: Human writes PRD in the UI Human approves the PRD System bundles PRD + context + criteria Bundle deploys to GitHub as a commit Agent reads bundle and implements Each step is visible. You can inspect the bundle before the agent touches it. You can see exactly what context it received. This traceability is the feature. When something goes wrong, you can ask: was the PRD unclear, or did the agent misinterpret it? The bundle is the evidence."
    },
    {
      "title": "Dream Machine begins",
      "date": "2026-01-20",
      "summary": "Building the infrastructure for AI-assisted development.",
      "url": "https://jlamb.dev/posts/dream-machine-begins/",
      "path": "/posts/dream-machine-begins/",
      "slug": "dream-machine-begins",
      "content_html": "<p>Dream Machine started as a question: what if you could describe what you want and have an agent build it?</p>\n<p>Not a chatbot that suggests code. Not an autocomplete that guesses your next line. A system that takes requirements, reasons about them, and produces working software.</p>\n<p>The initial commit was infrastructure. Next.js for the UI. Prisma for the database. Docker for sandboxing. GitHub Apps for authentication.</p>\n<p>Each choice was about control. Next.js because it runs anywhere. Prisma because migrations are explicit. Docker because agents need isolation. GitHub Apps because installation-level access beats personal tokens.</p>\n<p>Phase 4 was making the pieces talk to each other. The UI creates PRDs. The backend packages them. The sandbox runs agents. GitHub receives the output.</p>\n<p>The dream is simple: describe the machine you want, then watch it get built. The infrastructure is what makes that possible.</p>\n",
      "content_text": "Dream Machine started as a question: what if you could describe what you want and have an agent build it? Not a chatbot that suggests code. Not an autocomplete that guesses your next line. A system that takes requirements, reasons about them, and produces working software. The initial commit was infrastructure. Next.js for the UI. Prisma for the database. Docker for sandboxing. GitHub Apps for authentication. Each choice was about control. Next.js because it runs anywhere. Prisma because migrations are explicit. Docker because agents need isolation. GitHub Apps because installation-level access beats personal tokens. Phase 4 was making the pieces talk to each other. The UI creates PRDs. The backend packages them. The sandbox runs agents. GitHub receives the output. The dream is simple: describe the machine you want, then watch it get built. The infrastructure is what makes that possible."
    },
    {
      "title": "Why static is still enough",
      "date": "2026-01-15",
      "summary": "The simplest pipeline is the one you can explain.",
      "url": "https://jlamb.dev/posts/why-static/",
      "path": "/posts/why-static/",
      "slug": "why-static",
      "content_html": "<p>Static sites are not a nostalgia act. They are a refusal to let the tool decide the shape of your thinking.</p>\n<p>When you can trace every step from a Markdown file to a deployed page, you can trust the system. That is the whole point.</p>\n",
      "content_text": "Static sites are not a nostalgia act. They are a refusal to let the tool decide the shape of your thinking. When you can trace every step from a Markdown file to a deployed page, you can trust the system. That is the whole point."
    }
  ]
}
